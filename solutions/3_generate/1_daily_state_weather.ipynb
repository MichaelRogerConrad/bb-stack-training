{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the daily weather per state\n",
    "We have all the weather data, but we would like to calculate the weather data per day and per state. We will use that later on to join the weather data with the sales data so we will know what kind of weather it was when a sales transaction was made. \n",
    "\n",
    "This requires us to modify the data we already have in our master store and generate *views* of or data. In this case we will generate the **daily_weather_per_state** view.\n",
    "\n",
    "## Reading the data\n",
    "But first let's get started by loading the master weather data. The data is stored in a CSV file, but natively Spark has no handlers to talk to CSV's directly. This means we need to read the data as a *textFile*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather_data = sc.textFile(\"/data/master/weather\")\n",
    "weather_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so as we can see using the take(1) action, we now have a list of lines, but that is still not easy to work with. We will need to split the line using the field delimiter **\\t**. \n",
    "\n",
    "We can use the **rdd.map(fn)** transformation to iterate over the elements in a collection and perform a function on them at the same time. The **fn** argument in that function is actually a lambda expression. Lambda expressions in python are written as:\n",
    "\n",
    "**lambda** argument_1, ..., argument_n: *do_something*\n",
    "\n",
    "with a map transformation, the result returned by *do_something* in a lambda expression is used to replace the element in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather_records = weather_data.map(lambda line: line.split(\"\\t\"))\n",
    "weather_records.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a key\n",
    "The next step would be to generate a key for each record. In our case, we want to have a key including the data and the state, since we want to end up with one record per day per state.\n",
    "\n",
    "The format of the key in our case looks like **yyyymmdd-state**, meaning for a record with the following data:\n",
    "\n",
    "['2006', '1', '1', 'AB', 'PRCP', '4.57089552238806']\n",
    "\n",
    "our key would be '20060201-AB'. Keep in mind that the month is 0 based, so you will need to increment it in order to get the real month index.\n",
    "    \n",
    "For this we will need to perform a block of code, something that is not directly possible within a lambda expression. But it is allowed to call a method in a lambda expression so we will write our code in a method and point the lambda expression to our method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weather_key(record) :\n",
    "    res = str(record[0]);\n",
    "    res += min_two_digits(str(int(record[1]) +1));\n",
    "    res += min_two_digits(str(record[2]));\n",
    "    res += str('-');\n",
    "    res += str(record[3]);\n",
    "    return res;\n",
    "\n",
    "def min_two_digits(data):\n",
    "    return('00' + str(data))[-2:];\n",
    "\n",
    "weather = weather_records.map(lambda record: (weather_key(record), record))\n",
    "weather.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking all weather facts together\n",
    "We now have a collection of weather facts, but each fact is still an element of its own. We will need to take the weather facts together by their key so we can create records out of them.\n",
    "\n",
    "An RDD has an aggregateByKey(initVal, sequenceFn, combineFn) function we can use for this. The function takes an initial value, a sequence function and a combine function. The sequence function is used to sequentially add data to the initial value. Since the aggergation will be executed in a different manner, there will be several nodes starting with the initial value and running through a part of the data. This will result in several different results, one for each partition of the data. The combine function is used to merge these partition results together into one value.\n",
    "\n",
    "We want to create our record in the form of a dictionary, a key/value like data structure. An empty dictionary can be created using **{}** and data can be assigned to it using **myDict['field_name'] = fieldValue**. The value of a dictionary element can be retrieved using **myDict['field_name']**.\n",
    "\n",
    "You can use the **merge_two_dicts(dict1, dict2)** to combine two dictionaries into a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_two_dicts(x, y):\n",
    "    '''Given two dicts, merge them into a new dict as a shallow copy.'''\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def append_to_map(m, element):\n",
    "    m['date'] = str(element[0]) + '-' + min_two_digits(int(element[1]) + 1) + '-' + min_two_digits(element[2]);\n",
    "    m[element[4]] = float(element[5]);\n",
    "    return m;\n",
    "    \n",
    "seqFunc = (lambda r, v: append_to_map(r, v))\n",
    "combFunc = (lambda r1, r2: merge_two_dicts(r1, r2))\n",
    "weather = weather.aggregateByKey({}, seqFunc, combFunc)\n",
    "weather.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the result\n",
    "Well done! We have the result we wanted and since we do not want to loose it, we will store it onto hdfs.\n",
    "\n",
    "It is a good practice to store all your views under a same folder structure. In our case this would be **/data/views** and the name of this view would be **daily_weather_per_state**.\n",
    "\n",
    "We can choose out of different formats in which to store the data, but for now we will go with a sequence file. An RDD has a **saveAsSequenceFile(path)** method we can use for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weather.saveAsSequenceFile('/data/views/daily_weather_per_state')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.4.1)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
