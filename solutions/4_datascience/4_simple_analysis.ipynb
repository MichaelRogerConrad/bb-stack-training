{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Data Analysis\n",
    "\n",
    "During your data processing tasks, you might feel the need to get some insights into the data at hand. For this we can make use of Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "Let's say we want to get insights into the daily_weather_sales_per_state view we generated earlier. We stored that view using a CSV format at /data/views/daily_weather_sales_per_state. What you might remember as well was the difficulty we had with reading CSV formatted data previously. We had to read the data line by line, splitting the line and converting it into the format we wanted to work with and eventually still ending up with one big array.\n",
    "\n",
    "Well, The guys from databricks helped us out a bit by providing a spark-csv package. We can use this package to read CSV formatted data as a DataFrame. Once we have the data loaded, we can do all kind of things to it.\n",
    "\n",
    "The first step when using spark-csv is to describe the schema of the data. When working with CSV-formatted data, there might be a header available as the first line holding the column names. But since our data has been generated in a distributed way, there is no knowing what the actual column names are. So in order to load the data we have to describe it first.\n",
    "\n",
    "A custom schema, or a StructType can be created by providing a list of StructField's to it. A StructField is an actual column of the dataset, describing a field name, field type and a boolean to indicate if the field value could be *null*.\n",
    "\n",
    "In fact, a schema could look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mySchema = StructType([ \\\n",
    "    StructField(\"field_1\", StringType(), True), \\\n",
    "    StructField(\"field_2\", IntegerType(), True), \\\n",
    "                      \n",
    "    StructField(\"field_n\", FloatType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let us define the schema for our data and store it in a variable called **sales_schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales_schema = StructType([ \\\n",
    "    StructField(\"key\", StringType(), True), \\\n",
    "    StructField(\"customer_age\", IntegerType(), True), \\\n",
    "    StructField(\"customer_gender\", StringType(), True), \\\n",
    "    StructField(\"customer_key\", IntegerType(), True), \\\n",
    "    StructField(\"customer_marital_status\", StringType(), True), \\\n",
    "    StructField(\"customer_name\", StringType(), True), \\\n",
    "    StructField(\"customer_state\", StringType(), True), \\\n",
    "    StructField(\"date\", StringType(), True), \\\n",
    "    StructField(\"employee_gender\", StringType(), True), \\\n",
    "    StructField(\"employee_job_title\", StringType(), True), \\\n",
    "    StructField(\"employee_key\", IntegerType(), True), \\\n",
    "    StructField(\"employee_name\", StringType(), True), \\\n",
    "    StructField(\"employee_state\", StringType(), True), \\\n",
    "    StructField(\"price\", FloatType(), True), \\\n",
    "    StructField(\"product_category\", StringType(), True), \\\n",
    "    StructField(\"product_department\", StringType(), True), \\\n",
    "    StructField(\"product_description\", StringType(), True), \\\n",
    "    StructField(\"product_key\", IntegerType(), True), \\\n",
    "    StructField(\"product_price\", FloatType(), True), \\\n",
    "    StructField(\"product_version\", IntegerType(), True), \\\n",
    "    StructField(\"quantity\", FloatType(), True), \\\n",
    "    StructField(\"store_key\", IntegerType(), True), \\\n",
    "    StructField(\"store_name\", StringType(), True), \\\n",
    "    StructField(\"store_state\", StringType(), True), \\\n",
    "    StructField(\"tender_type\", StringType(), True), \\\n",
    "    StructField(\"transaction\", StringType(), True), \\\n",
    "    StructField(\"transaction_type\", StringType(), True), \\\n",
    "    StructField(\"rainfall\", FloatType(), True), \\\n",
    "    StructField(\"temp_avg\", FloatType(), True), \\\n",
    "    StructField(\"temp_max\", FloatType(), True), \\\n",
    "    StructField(\"temp_min\", FloatType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was quite a bit of code there. luckily you will only have to do that once. Now we do have the schema, but we have not loaded the data yet. The **sqlContext** has a **sqlContext.read.load(path, options)** function we can use to load the data, but that will not use the CSV format by default. We can manipulate the way the data is read by calling an optional **format(_format_)** function before calling **load(path, options)**. The format we would use is called **com.databricks.spark.csv**.\n",
    "\n",
    "And then there is the schema which we will need to provide when loading the data. You can do so by passing **schema=sales_schema** in the options part of the load function.\n",
    "\n",
    "Let us create a new DataFrame with the contents of the CSV file. Once the DataFrame has been created you can call **take(_n_)** to display the first n records, just like you did with an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .load('/data/views/daily_weather_sales_per_state', schema = sales_schema)\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wicked! We created our first DataFrame!\n",
    "\n",
    "## Working with data\n",
    "Let's start simple by showing the top 5 stores with the most sales out of our DataFrame. This will require us to group our data by store_key and counting the sales records for each store. After that we will sort the results descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_performing_stores = df \\\n",
    "    .groupBy('store_key') \\\n",
    "    .count() \\\n",
    "    .sort(desc('count'))\n",
    "    \n",
    "high_performing_stores.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's move to something more difficult. We have a full collection of transactions, but we don't know the total value of a single transaction. So let us add a column to our DataFrame to calculate the transaction total. DataFrame has a function called **withColumn(_name_, _expression_)** that can help us to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transaction_totals = df \\\n",
    "    .withColumn('total', df.price * df.quantity) \\\n",
    "    .select('date', 'transaction', 'customer_key', 'customer_marital_status', 'customer_age', 'customer_gender', 'total') \\\n",
    "\n",
    "transaction_totals = transaction_totals \\\n",
    "    .withColumn('customer_age_bucket', ceil(transaction_totals.customer_age / 10))\n",
    "    \n",
    "transaction_totals.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know the value of a single transaction,  but we would like to know the total value of the goods bought by the customer on a single day. For this we can group our transaction totals by date and customer and calculate the sum of all totals.\n",
    "\n",
    "If you want to do more than one aggregation, you can make use of the DataFrame's **agg(_expressions_)** function. You can pass columns or column expressions to the _expressions_ argument. Common column expressions are **sum(_field_)**, **count(_field_)**, **avg(_field_)**, **min(_field_)** and **max(_field_)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt = transaction_totals \\\n",
    "    .groupBy('date', 'customer_key', 'customer_marital_status', 'customer_age', 'customer_age_bucket', 'customer_gender') \\\n",
    "    .agg(sum('total').alias('total'))\n",
    "    \n",
    "tt.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting some customer information\n",
    "With the use of the transactions and the transaction totals we can create an image of our customers. Let's start by figuring out the average amount spent by a customer on each visit. and let's group that by customer_marital_status to see if there are more customers with a certain marital state buying our goods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "marital_status_visits = tt \\\n",
    "    .groupBy('customer_marital_status') \\\n",
    "    .agg( \\\n",
    "        sum('total').alias('total'), \\\n",
    "        count('customer_key').alias('visits'), \\\n",
    "        avg('total').alias('avg_per_visit') \\\n",
    "    ) \\\n",
    "    .sort(desc('avg_per_visit'))\n",
    "    \n",
    "marital_status_visits.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done! Let's do the same thing again, but this time we group by customer_gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gender_visits = tt \\\n",
    "    .groupBy('customer_gender') \\\n",
    "    .agg( \\\n",
    "        sum('total').alias('total'), \\\n",
    "        count('customer_key').alias('visits'), \\\n",
    "        avg('total').alias('avg_per_visit') \\\n",
    "    ) \\\n",
    "    .sort(desc('avg_per_visit'))\n",
    "    \n",
    "gender_visits.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And since we also have the customer_age, it would be nice to see what age our customers are. But here is the catch; we want to group by age ranges instead of the actual age of a customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transaction_totals_with_age_bucket = transaction_totals \\\n",
    "    .withColumn('customer_age_bucket', ceil(transaction_totals.customer_age / 10))\n",
    "    \n",
    "tt_with_age_bucket = transaction_totals \\\n",
    "    .groupBy('date', 'customer_key', 'customer_marital_status', 'customer_age', 'customer_age_bucket', 'customer_gender') \\\n",
    "    .agg(sum('total').alias('total'))\n",
    "    \n",
    "age_visits = tt_with_age_bucket \\\n",
    "    .groupBy('customer_age_bucket') \\\n",
    "    .agg( \\\n",
    "        sum('total').alias('total'), \\\n",
    "        count('customer_key').alias('visits'), \\\n",
    "        avg('total').alias('avg_per_visit') \\\n",
    "    ) \\\n",
    "    .sort(desc('customer_age_bucket'))\n",
    "    \n",
    "age_visits.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating customer profiles\n",
    "Let's group everything we know about our customers to see which customer segment is generating the most turnover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "customer_segments = tt \\\n",
    "    .groupBy(tt.customer_gender, tt.customer_marital_status, tt.customer_age_bucket) \\\n",
    "    .agg( \\\n",
    "        sum('total').alias('total'), \\\n",
    "        count('customer_key').alias('visits'), \\\n",
    "        avg('total').alias('avg_per_visit') \\\n",
    "    ) \\\n",
    "    .sort(desc('total'))\n",
    "    \n",
    "customer_segments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing our results\n",
    "DataFrames can also be stored to HDFS using several different output formats. The DataFrame provides a **write** member with functions for storing in json, orc or parquet. For this our case we will use parquet to store the customer_segments as a new view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "customer_segments.write.parquet('/data/views/customer_segments')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.4.1)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
