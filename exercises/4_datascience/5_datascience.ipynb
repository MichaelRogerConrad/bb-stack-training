{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science\n",
    "[Data Science](https://en.wikipedia.org/wiki/Data_science) is an **interdisciplinary field** about processes and systems to **extract knowledge or insights from data** in various forms, either structured or unstructured.\n",
    "\n",
    "The term should read **data craftsmenship** because it is more about expertise and experience in applying knowlegde and algorithms from various fields and sciences. \n",
    "\n",
    "Let's not call it an art :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "We want to build a **recommendation engine** based on the sales data we collected. A recommendation engine allows you to recommend new products to users which he most likely be interested it. The recommender can work in 2 ways: \n",
    "\n",
    "1. **Product perspective**: You are buying or liking or looking at this product, but we can also recommend these other products. \n",
    "1. **User perspective**: Other users like you, prefer these products too.\n",
    "\n",
    "But do mind '[how you target](http://www.nytimes.com/2012/02/19/magazine/shopping-habits.html)' ;-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark MLlib\n",
    "Spark's [Machine Learning Library (MLlib)](http://spark.apache.org/docs/latest/mllib-guide.html#sparkml-high-level-apis-for-ml-pipelines) has the necessary tools build recommenders. \n",
    "\n",
    "We will use the *amount of products sold per customer* as an **expression of preference for a given product**. \n",
    "\n",
    "If we skimm through MLlib's available algorithms, [collaborative filtering](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html) using [alternating least squares (ALS)](http://dl.acm.org/citation.cfm?id=1608614) seems very appropriate, because you can also train the algorithm using only **implicit feedback** (*amount of products sold*). \n",
    "\n",
    "So let's pick-up where we started in the previous chapter: read the **sales dataframe**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(key=u'20060416-LA', customer_age=34, customer_gender=u'Male', customer_key=8255, customer_marital_status=u'Married', customer_name=u'Kevin W. Perkins', customer_state=u'FL', date=u'2006-04-16T00:00:00.000000Z', employee_gender=u'Female', employee_job_title=u'Greeter', employee_key=7768, employee_name=u'Carla King', employee_state=u'TX', price=361.0, product_category=u'Misc', product_department=u'Gifts', product_description=u'Brand #2402 greeting cards', product_key=790, product_price=137.0, product_version=1, quantity=10.0, store_key=115, store_name=u'Store115', store_state=u'LA', tender_type=u'Check', transaction=u'3281511', transaction_type=u'purchase', rainfall=1.0978261232376099, temp_avg=185.42857360839844, temp_max=219.0625, temp_min=95.76841735839844)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.mllib.recommendation import ALS, MatrixFactorizationModel, Rating\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "sales_schema = StructType([ \\\n",
    "    StructField(\"key\", StringType(), True), \\\n",
    "    StructField(\"customer_age\", IntegerType(), True), \\\n",
    "    StructField(\"customer_gender\", StringType(), True), \\\n",
    "    StructField(\"customer_key\", IntegerType(), True), \\\n",
    "    StructField(\"customer_marital_status\", StringType(), True), \\\n",
    "    StructField(\"customer_name\", StringType(), True), \\\n",
    "    StructField(\"customer_state\", StringType(), True), \\\n",
    "    StructField(\"date\", StringType(), True), \\\n",
    "    StructField(\"employee_gender\", StringType(), True), \\\n",
    "    StructField(\"employee_job_title\", StringType(), True), \\\n",
    "    StructField(\"employee_key\", IntegerType(), True), \\\n",
    "    StructField(\"employee_name\", StringType(), True), \\\n",
    "    StructField(\"employee_state\", StringType(), True), \\\n",
    "    StructField(\"price\", FloatType(), True), \\\n",
    "    StructField(\"product_category\", StringType(), True), \\\n",
    "    StructField(\"product_department\", StringType(), True), \\\n",
    "    StructField(\"product_description\", StringType(), True), \\\n",
    "    StructField(\"product_key\", IntegerType(), True), \\\n",
    "    StructField(\"product_price\", FloatType(), True), \\\n",
    "    StructField(\"product_version\", IntegerType(), True), \\\n",
    "    StructField(\"quantity\", FloatType(), True), \\\n",
    "    StructField(\"store_key\", IntegerType(), True), \\\n",
    "    StructField(\"store_name\", StringType(), True), \\\n",
    "    StructField(\"store_state\", StringType(), True), \\\n",
    "    StructField(\"tender_type\", StringType(), True), \\\n",
    "    StructField(\"transaction\", StringType(), True), \\\n",
    "    StructField(\"transaction_type\", StringType(), True), \\\n",
    "    StructField(\"rainfall\", FloatType(), True), \\\n",
    "    StructField(\"temp_avg\", FloatType(), True), \\\n",
    "    StructField(\"temp_max\", FloatType(), True), \\\n",
    "    StructField(\"temp_min\", FloatType(), True) \\\n",
    "])\n",
    "\n",
    "df = sqlContext.read \\\n",
    "    .format('com.databricks.spark.csv') \\\n",
    "    .load('/data/views/daily_weather_sales_per_state', schema = sales_schema)\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn our sales data into customer preferences\n",
    "Now, build the ratings-like customer's **preference dataframe** but *aggregating the amount of units sold per product*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(customer_key=33843, product_key=8786, rating=5.0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preferences = df \\\n",
    "    .groupBy('customer_key', 'product_key') \\\n",
    "    .agg(sum('quantity').alias('rating'))\n",
    "\n",
    "preferences.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(customer_key=33843, product_key=6156, rating=1.0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preferences.where('customer_key = 33843 and product_key = 6156').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Rating(user=33843, product=6156, rating=1.0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ALS requires a dataset of Ratings\n",
    "ratings = preferences.map(lambda l: Rating(int(l[0]), int(l[1]), float(l[2])))\n",
    "ratings.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the algorithm\n",
    "Next we will have to train a **model using the ALS algorithm**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the ratings dataset into a training (75%) and test (25%) dataset \n",
    "splits = ratings.randomSplit([0.75, 0.25], 11)\n",
    "\n",
    "training = splits[0].cache()\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS\n",
    "model = ALS.train(training, rank=10, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 128.438936634\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "testdata = test.map(lambda p: (p[0], p[1]))\n",
    "predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = test.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save and load model\n",
    "model.save(sc, \"/data/models/als_1\")\n",
    "sameModel = MatrixFactorizationModel.load(sc, \"/data/models/als_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the algorithm for implicit feedback\n",
    "Next we will train a model using ALS but for *implicit feedback*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the recommendation model using ALS\n",
    "model = ALS.trainImplicit(training, rank=10, iterations=10, lambda_=0.01, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 38.4803284453\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on training data\n",
    "testdata = test.map(lambda p: (p[0], p[1]))\n",
    "predictions = model.predictAll(testdata).map(lambda r: ((r[0], r[1]), r[2]))\n",
    "ratesAndPreds = test.map(lambda r: ((r[0], r[1]), r[2])).join(predictions)\n",
    "MSE = ratesAndPreds.map(lambda r: (r[1][0] - r[1][1])**2).mean()\n",
    "print(\"Mean Squared Error = \" + str(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 1.4.1)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
